{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2eb5e3-df51-42f6-98c2-e4a016e443b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from transformers import AutoTokenizer, TrainingArguments, TrainerCallback, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from generation import generate_task\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "answer_token = 31984\n",
    "\n",
    "model = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba-130m\", dtype=torch.bfloat16, device=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids = inputs.pop(\"input_ids\")\n",
    "        # batched generation\n",
    "        lm_logits = model(input_ids).logits\n",
    "\n",
    "        labels = input_ids.to(lm_logits.device)\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        # cut out the task part (the part before \"answer\")\n",
    "        cut_shift_logits = []\n",
    "        cut_labels = []\n",
    "        for ex_shift_logits, ex_labels in zip(shift_logits, labels):\n",
    "            # find the indexes of the \"answer\" token\n",
    "            answer_index = torch.where(ex_labels == answer_token)[0]\n",
    "            answer_index = int(answer_index)\n",
    "            # cut out the task part\n",
    "            cut_shift_logits.append(ex_shift_logits[answer_index:])\n",
    "            cut_labels.append(ex_labels[answer_index:])\n",
    "\n",
    "        # calculate loss only for the tokens after \"answer\"\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        lm_loss = loss_fct(\n",
    "            torch.cat(cut_shift_logits),\n",
    "            torch.cat(cut_labels),\n",
    "        )\n",
    "\n",
    "        return lm_loss\n",
    "\n",
    "    def save_model(self, output_dir, _internal_call):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        torch.save(self.model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    def log(self, logs):\n",
    "        pass  # Override to do nothing and avoid printing logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_examples, sequence_length, mask):\n",
    "        texts = []\n",
    "        for _ in range(num_examples):\n",
    "            task, normal_reasoning, masked_reasoning = generate_task(sequence_length)\n",
    "            completion = masked_reasoning if mask else normal_reasoning\n",
    "            texts.append(f\"{task}\\nanswer\\n{completion}\")\n",
    "        \n",
    "        tokenized = tokenizer(texts, padding=True)[\"input_ids\"]\n",
    "        tensors = [torch.LongTensor(tok) for tok in tokenized]\n",
    "\n",
    "        self.input_ids = tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return dict(input_ids=self.input_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval correctness\n",
    "def eval_correctness(all_input_ids):\n",
    "    # todo parallelize\n",
    "    is_corrects = []\n",
    "    for full_input_ids in all_input_ids:\n",
    "        task, target_reasoning = tokenizer.decode(full_input_ids).split(\"answer\")\n",
    "        task += \"answer\"\n",
    "\n",
    "        task_tokens = tokenizer(task, return_tensors=\"pt\")\n",
    "        input_ids = task_tokens.input_ids.to(device=\"cuda\")\n",
    "        attn_mask = task_tokens.attention_mask.to(device=\"cuda\")\n",
    "        max_length = len(full_input_ids)\n",
    "\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            cg=True,\n",
    "            temperature=1,\n",
    "        )\n",
    "        text = tokenizer.decode(out[0])\n",
    "        # print(text)\n",
    "        # print(\"--- correct: ---\")\n",
    "        # print(target_reasoning.strip())\n",
    "        model_reasoning = text.split(\"answer\")[-1]\n",
    "        is_correct = model_reasoning.strip() == target_reasoning.strip()\n",
    "        is_corrects.append(is_correct)\n",
    "\n",
    "    perc_correct = sum(is_corrects) / len(is_corrects)\n",
    "    return perc_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b0806-bc08-4c14-b5b1-0043a6007bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "mask = True\n",
    "\n",
    "trainer = MambaTrainer(\n",
    "    model=model,\n",
    "    # train_dataset=train_dataset,\n",
    "    # eval_dataset=eval_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    args=TrainingArguments(\n",
    "        disable_tqdm=True,  # This disables the progress bars\n",
    "        learning_rate=5e-4,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=16,\n",
    "        gradient_accumulation_steps=1,\n",
    "        dataloader_num_workers=2,\n",
    "        optim=\"adamw_torch\",\n",
    "        output_dir=\"out\",\n",
    "        # logging_steps=10,\n",
    "        weight_decay=1e-2,\n",
    "        # evaluation_strategy=\"steps\",\n",
    "        # eval_steps=10,\n",
    "        # save_strategy=\"epoch\"\n",
    "    ),\n",
    "    # data_collator=data_collator\n",
    "    # data_collator=DataCollatorForSFTDataset(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "# for _ in range(20):\n",
    "while True:\n",
    "    train_dataset=MyDataset(num_examples=160, sequence_length=sequence_length, mask=mask)\n",
    "    eval_dataset=MyDataset(num_examples=20, sequence_length=sequence_length, mask=mask)\n",
    "\n",
    "    trainer.train_dataset = train_dataset\n",
    "    trainer.train()\n",
    "\n",
    "    perc_correct = eval_correctness(eval_dataset[:20][\"input_ids\"])\n",
    "    # print(f\"correct answers: {perc_correct:4.0%}  seq.len.: {sequence_length}\")\n",
    "    num_correct = int(perc_correct * 20)\n",
    "    accuracy_bar = \"«\" + \"█\" * num_correct + \" \" * (20 - num_correct) + \"»\"\n",
    "    print(f\"seq.len.: {sequence_length:3}   \" + accuracy_bar)\n",
    "    if perc_correct >= 0.9:\n",
    "        sequence_length += 1\n",
    "    if sequence_length > 40:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_correctness(eval_dataset[:20][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = eval_dataset[0:16][\"input_ids\"]\n",
    "examples = torch.stack(examples).to(\"cuda\")\n",
    "trainer.compute_loss(model, dict(input_ids=examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb089c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_token = 31984\n",
    "\n",
    "examples = eval_dataset[0:3][\"input_ids\"]\n",
    "input_ids = torch.stack(examples).to(\"cuda\")\n",
    "# batched generation\n",
    "lm_logits = model(input_ids).logits\n",
    "\n",
    "labels = input_ids.to(lm_logits.device)\n",
    "shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "labels = labels[:, 1:].contiguous()\n",
    "\n",
    "# cut out the task part (the part before \"answer\")\n",
    "cut_shift_logits = []\n",
    "cut_labels = []\n",
    "for ex_shift_logits, ex_labels in zip(shift_logits, labels):\n",
    "    # find the indexes of the \"answer\" token\n",
    "    answer_index = torch.where(ex_labels == answer_token)[0]\n",
    "    answer_index = int(answer_index)\n",
    "    # cut out the task part\n",
    "    cut_shift_logits.append(ex_shift_logits[answer_index:])\n",
    "    cut_labels.append(ex_labels[answer_index:])\n",
    "\n",
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "lm_loss = loss_fct(\n",
    "    torch.cat(cut_shift_logits),\n",
    "    torch.cat(cut_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480789c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_pred):\n",
    "#     # predictions, labels = eval_pred\n",
    "#     # # Assuming predictions are token IDs and have already been trimmed of padding\n",
    "#     # avg_num_tokens = predictions.shape[-1]  # Get the sequence length dimension\n",
    "#     # return {\"avg_num_tokens\": avg_num_tokens}\n",
    "#     return {\"dummy\": 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorForSFTDataset(object):\n",
    "#     \"\"\"\n",
    "#     Collate examples for supervised fine-tuning.\n",
    "#     \"\"\"\n",
    "\n",
    "#     tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "#     def __call__(self, instances):\n",
    "#         input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"input_ids\"))\n",
    "#         input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "#         labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "#         return dict(\n",
    "#             input_ids=input_ids,\n",
    "#             # labels=labels,\n",
    "#             # attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
